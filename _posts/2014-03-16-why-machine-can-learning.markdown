---
layout: post
title:  "why machine can learning"
date:   2014-03-16 10:25:37
categories: machine learning 
---

## 为什么机器可以学习？##
机器学习(machine learning)在工程上体现为通过学习部分数据（样本）对整体或未来的数据进行预测，如通过学习用户标记的垃圾邮件来预测其他或未来的邮件是否为垃圾邮件。

一个直接的问题是为什么可以通过局部预测全部（为什么机器可以学习）？

### 来自统计学的理论 ###
抛开机器学习这个话题，统计学上很早就遇到类似的问题，比如美国大选，对一小部分人进行民意调查来预测最终的大选结果？这也是一个以局部来预测全部的问题。

将问题抽象一下，使用coursera中machine learning foundation的例子，假设有一个罐子里面有红色和绿色两种弹珠，其中红色的比例为u，从罐子中拿出N个弹珠，红色的比例为v, 为v和u有多大的概率相等（即有大的概率通过局部准确预测全部)？

如果u = 0.9,  N = 10, 那么v = u = 0.9等价于取出的10个中有9个是红色的弹珠，可以计算概率如下

$$
\binom{9}{10}u^{9}(1-u) \approx 0.39
$$

嗯，如果我们并不要求u=v, 而是放宽要求，v和u大致相等即可，譬如v的范围为[u-0.1, u+0.1]，即[0.8, 1],  也即v >= 0.8, N = 10的情况下，概率为

$$
\binom{10}{10}u^{10} + \binom{9}{10}u^{9}(1-u) + \binom{8}{10}u^{8}(1-u)^{2} \approx 0.93
$$

可以看到，u和v大致相等的概率比要求u=v的概率提高很多。也就是说|u - v| <= 0.1的概率大概为0.93， 反之就是 |u-v| > 0.1的概率为 1-0.93=0.07

另一方面，如果我们能够看到更多的局部数据，如N = 100, 则对应的概率为

$$
\binom{100}{100}u^{100} + ...  + \binom{80}{100}u^{80}(1-u)^{20} \approx 0.99
$$

可以看出，N的增大使得u和v大致相等的概率也增大了。对应|u-v]>0.1的概率为0.01，即超出误差范围的概率更小了。

实际上，在统计学里，有一个著名的理论*** hoeffding inequality ***来衡量误差范围和超出误差范围的概率，定义如下

$$
   P[|u - v| > \varepsilon ] < 2exp(-2\varepsilon ^2 N)
$$

在u未知的情况下，虽然不能求出具体u和v误差的大小，却可以给出超出误差范围出现的概率边界。以上一个例子来看

$$
   P[|u-v| > 0.1] < 2*exp(-2*0.01*100) = 0.27
$$

而上个例子的计算的真实概率为0.01，确实在hoeffding inequality给定的范围之内。


### 和机器学习的关系 ###
回到机器学习问题上来，需要解决的是保证在局部数据（样本）表现良好的模型在整体数据上也能表现良好。我们假设每个样本都是一个弹珠，如果学习后预测的值h(x)与实际值f(x)不同，这个弹珠就是红色，否则为绿色，这样预测的错误率就是上一节中红色弹珠的比例。在样本中的错误率为v，在全部数据中的错误率为u，这样通过hoeffdin inequality可以估算模型在实际中（在全部数据上）的表现。

这里的v我们叫做in-sample error

$$
   E_{in}(h) = \frac{1}{N}\sum_{n=1}^{N}sign(h(x_{n})\neq f(x_{n}))
$$

相应的u叫做out-of-sample error。

对于一个固定的预测函数h(x)(我们可以将这个函数称为对真实情况的一个假设)而言，in sample error与out of sample error的关系满足

$$
 P[|{E}_{in} - {E}_{out}| > \varepsilon ] < 2exp(-2\varepsilon ^2 N)
$$

实际上在机器学习时，我们在分析样本数据前时是无法确定固定的假设或模型h（而是需要根据不同的样本数据选择表现最好的h），这会之前讨论的固定h有什么不同？

hoeffding inequality成立的一个重要前提是局部（样本）数据必须是随机独立地从全部数据中抽样获得，换言之，局部和整体关于u或v的分布是一致的。

假设我们有1000个硬币，抛出后为正面的概率是1/2,每个硬币抛10次，然后选择两个硬币统计出现正面的次数，1)随机选一个 2)选择出现正面次数最少的一个。如果我们重复这个试验若干次，我们会发现两种选择1)和2)的出现正面的次数分布完全不同(可以直接看直方图)。

这里正面出现的次数就等同于前面提到的v(in-sample error)， 2)的选择等同于机器学习算法中选择表现最好的假设（模型）， 这种选择已经破坏了hoeffding inequality要求的v数据分布保持不变的前提。

我们现在需要重新考虑引入假设选择过程后如何保证hoeffding inequality也能适应。假设我们有M个候选假设（预测函数），机器学习算法会根据样本数据选择表现最好的假设g，因为

$$
\left |E_{in}(g) - E_{out}(g) \right |  > \varepsilon  \Rightarrow  \left |E_{in}(h_{1}) - E_{out}(h_{1}) \right | > \varepsilon  \; or \; \left |E_{in}(h_{2}) - E_{out}(h_{2}) \right | > \varepsilon  \; or \; ... \;or \; \left |E_{in}(h_{M}) - E_{out}(h_{M}) \right | > \varepsilon 
$$

概率上有两个定律

$$
if \; A \Rightarrow B \; , then \; P[A] \leq P[B] 
$$

$$
P[A_{1}\,or\,A_{2}\,or\,...or\,A_{M}] \leq P[A_{1}] + P[A_{2}] + ... + P[A_{M}]
$$

根据这两个定律，我们可以推导如下

$$
P[\left |E_{in}(g) - E_{out}(g)\right | > \varepsilon ] \leq P[\left |E_{in}(h_{1}) - E_{out}(h_{1}) \right | > \varepsilon  \; or \; P[\left |E_{in}(h_{2}) - E_{out}(h_{2}) \right | > \varepsilon  \; or \; ... \;or \; \left |E_{in}(h_{M}) - E_{out}(h_{M}) \right | > \varepsilon] 
$$

$$
P[\left |E_{in}(g) - E_{out}(g)\right | > \varepsilon ] \leq \sum_{m=1}^{M}P[\left |E_{in}(h_{m}) - E_{out}(h_{m}) \right |] \leq 2Mexp(-2\varepsilon ^2 N)
$$

我们从而得到了机器是否可以学习的预估公式，从这个不等式可以知道，机器学习到的模型在实际环境中是否可行与误差的容忍度，样本的数量N，以及假设的数目M（一般来说模型越复杂，假设数目越大）密切相关。


### 现实很骨感 ###

上面大致上回答了为什么机器可以学习的问题，但还没有完全解决现实中的问题，如果M无穷大怎么办？
一个简单的模型，如线性模型 ax + b，参数a，b能够产生的函数（假设）个数为无穷大，这时候误差的边界就失去了意义。

解决的思路是M虽然无穷大，样本的数量是有限的，那么对于样本预测的结果组合也是有限的，所以可以将无限的问题通过样本转化到有限的空间上，这是机器学习中的VC-dimension问题， 鉴于本文篇幅，留待下篇再述。



